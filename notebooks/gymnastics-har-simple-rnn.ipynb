{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "\n",
    "from numpy import dstack\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from path import Path\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading frames for 0/11300\n",
      "Loading frames for 1/11300\n",
      "Loading frames for 2/11300\n",
      "Loading frames for 3/11300\n",
      "Loading frames for 4/11300\n",
      "Loading frames for 5/11300\n",
      "Loading frames for 6/11300\n",
      "Loading frames for 7/11300\n",
      "Loading frames for 8/11300\n",
      "Loading frames for 9/11300\n",
      "Loading frames for 10/11300\n",
      "Loading frames for 11/11300\n",
      "Loading frames for 12/11300\n",
      "Loading frames for 13/11300\n",
      "Loading frames for 14/11300\n",
      "Loading frames for 15/11300\n",
      "Loading frames for 16/11300\n",
      "Loading frames for 17/11300\n",
      "Loading frames for 18/11300\n",
      "Loading frames for 19/11300\n",
      "Loading frames for 20/11300\n",
      "Loading frames for 21/11300\n",
      "Loading frames for 22/11300\n",
      "Loading frames for 23/11300\n",
      "Loading frames for 24/11300\n",
      "Loading frames for 25/11300\n",
      "Loading frames for 26/11300\n",
      "Loading frames for 27/11300\n",
      "Loading frames for 28/11300\n",
      "Loading frames for 29/11300\n",
      "Loading frames for 30/11300\n",
      "Loading frames for 31/11300\n"
     ]
    }
   ],
   "source": [
    "from helpers.classes.data_loader import DataLoader\n",
    "\n",
    "root_path = \"/Users/allarviinamae/EduWorkspace/openpose-jupyter-data-exploration/augmented-keypoints\"\n",
    "\n",
    "sample_dir_names = [n for n in os.listdir(root_path) if os.path.isdir(f\"{root_path}/{n}\")]\n",
    "\n",
    "samples = DataLoader.get_samples_list(sample_dir_names, root_path)\n",
    "\n",
    "y_labels = DataLoader.get_y_labels(sample_dir_names) # classifier labels, where 0 = backflip and 1 = flack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "\n",
    "# Print some sample dir names\n",
    "a = [print(sample_dir_name) for sdn_i, sample_dir_name in enumerate(sample_dir_names) if sdn_i < i]\n",
    "a = [print(y_label) for y_i, y_label in enumerate(y_labels) if y_i < i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.classes.padder import Padder\n",
    "\n",
    "padded_samples_list = Padder.get_padded_samples(samples)\n",
    "categorical_y_labels = DataLoader.get_categorical_y_labels(y_labels)\n",
    "\n",
    "padded_samples_ndarray = np.asarray(padded_samples_list)\n",
    "categorical_y_labels_ndarray = np.asarray(categorical_y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model(lstm_units, n_outputs, n_features, n_timesteps):\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    \n",
    "    input_shape = (n_timesteps, n_features)\n",
    "    print(f\"Model input shape {input_shape}\")\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(lstm_units, \n",
    "                        return_sequences=True,\n",
    "                        input_shape=input_shape))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(lstm_units, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs = 1, 5\n",
    "    \n",
    "    #n_steps_per_epoch = 360\n",
    "    \n",
    "    lstm_units = 2\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    \n",
    "    model = get_simple_rnn_model(lstm_units, n_outputs, n_features, n_timesteps)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    es_callback = EarlyStopping(monitor='val_loss',\n",
    "                                patience=3)\n",
    "    \n",
    "    history = model.fit(trainX,\n",
    "                        trainy,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=verbose,\n",
    "                        validation_split=0.33,\n",
    "                        #callbacks=[es_callback]\n",
    "                       )\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # fit network\n",
    "    #model.fit_generator(train_generator(trainX, trainy), steps_per_epoch=n_steps_per_epoch, epochs=epochs, verbose=verbose)\n",
    "    # evaluate model\n",
    "    #_, accuracy = model.evaluate_generator(train_generator(testX, testy), steps=len(testX), verbose=0)\n",
    "    \n",
    "    return history, accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, repeat):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"model-output/model-{repeat}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(f\"model-output/model-{repeat}.h5\")\n",
    "    \n",
    "    plot_model(model, to_file=f\"model-output/model-plot-{repeat}.png\", show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_samples_ndarray,\n",
    "                                                    categorical_y_labels_ndarray,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f\"X train len: {len(X_train)}, y train len:{len(y_train)} --- X test len:{len(X_test)}, y test len:{len(y_test)}\")\n",
    "\n",
    "train_loss_history = DataFrame()\n",
    "val_loss_history = DataFrame()\n",
    "\n",
    "train_accuracy_history = DataFrame()\n",
    "val_accuracy_history = DataFrame()\n",
    "\n",
    "model_history = []\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=5):\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    last_history = None\n",
    "    \n",
    "    for r in range(repeats):\n",
    "        history, score, model = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        model_history.append(model)\n",
    "        save_model(model, r)\n",
    "        \n",
    "        # story history\n",
    "        train_loss_history[str(r)] = history.history['loss']\n",
    "        val_loss_history[str(r)] = history.history['val_loss']\n",
    "        train_accuracy_history[str(r)] = history.history['accuracy']\n",
    "        val_accuracy_history[str(r)] = history.history['val_accuracy']\n",
    "        \n",
    "        score = score * 100.0\n",
    "        print('>#%d validation accuracy: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "        \n",
    "    # summarize results\n",
    "    summarize_results(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(train_loss_history['3'], color='blue', label='train')\n",
    "pyplot.plot(val_loss_history['3'], color='orange', label='validation')\n",
    "pyplot.title('LSTM model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "#pyplot.show()\n",
    "\n",
    "pyplot.savefig('model-output/model-train-vs-validation-loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(train_accuracy_history['3'], color='blue', label='train')\n",
    "pyplot.plot(val_accuracy_history['3'], color='orange', label='validation')\n",
    "pyplot.title('LSTM model train vs validation accuracy')\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "#pyplot.show()\n",
    "\n",
    "pyplot.savefig('model-output/model-train-vs-validation-accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sample_idx = 89\n",
    "model_history_idx = 3\n",
    "\n",
    "example_sample_dir_name = sample_dir_names[example_sample_idx]\n",
    "example_sample = padded_samples_ndarray[example_sample_idx]\n",
    "example_model = model_history[model_history_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(loaded_model, samples, sample_dir_names, y_labels):\n",
    "    ynew = loaded_model.predict_classes(samples)\n",
    "    # show the inputs and predicted outputs\n",
    "    for i in range(len(samples)):\n",
    "        pred_y = ynew[i]\n",
    "        actual_y = y_labels[i]\n",
    "    \n",
    "        same = False\n",
    "        if pred_y == actual_y:\n",
    "            same = True\n",
    "    \n",
    "        print(\"Name=%s, X=%s, Predicted=%s, Actual=%s, same=%s\" % (sample_dir_names[i], i, pred_y, actual_y, same))\n",
    "    \n",
    "predictions(example_model, padded_samples_ndarray, sample_dir_names, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in example_model.layers]\n",
    "\n",
    "print(example_sample_dir_name)\n",
    "print(example_sample.shape)\n",
    "print(layer_names)\n",
    "\n",
    "def activations(model, example_sample): \n",
    "    n_timesteps, n_features = example_sample.shape[0], example_sample.shape[1]\n",
    "    \n",
    "    print(f\"n_timesteps: {n_timesteps}, n_features: {n_features}\")\n",
    "    \n",
    "    x = np.zeros((1, n_timesteps, n_features))\n",
    "    \n",
    "    for t, timestep in enumerate(example_sample):\n",
    "        for f, feature in enumerate(timestep):\n",
    "            x[0, t, f] = example_sample[t][f]\n",
    "                \n",
    "    output = model.get_layer('simple_rnn_4').output\n",
    "    \n",
    "    f = K.function([model.input], [output])\n",
    "    \n",
    "    return f([x])[0][0]\n",
    "\n",
    "\n",
    "act = activations(model_history[model_history_idx], example_sample)\n",
    "print(act)\n",
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "from PIL import ImageDraw\n",
    "from IPython.display import Image\n",
    "\n",
    "def get_image(img, n_timesteps, img_idx, cell_size=48):\n",
    "    img_width = n_timesteps * 25\n",
    "    cell_size = int(img_width / n_timesteps)\n",
    "    \n",
    "    pil_image = PILImage.fromarray(img.astype(np.uint8))\n",
    "    \n",
    "    resized_pil_image = pil_image.resize((img_width, cell_size))\n",
    "    #resized_pil_image = pil_image\n",
    "    \n",
    "    draw = ImageDraw.Draw(resized_pil_image)\n",
    "    \n",
    "    for n_timestep in range(n_timesteps):\n",
    "        text = str((img_idx * 30) + n_timestep)\n",
    "        xy = (n_timestep * cell_size, 0)\n",
    "        \n",
    "        draw.text(xy, text)\n",
    "        \n",
    "    f = BytesIO()\n",
    "    resized_pil_image.save(f, 'png')\n",
    "    return Image(data=f.getvalue())\n",
    "\n",
    "def visualize_neurons(act, cell_size=48):\n",
    "    n_neurons = act.shape[1]\n",
    "    n_timesteps = act.shape[0]\n",
    "    \n",
    "    fill_value = 128\n",
    "    \n",
    "    img = np.full((n_neurons + 1, n_timesteps, 3), fill_value)\n",
    "    \n",
    "    # add 1 to each value in matrix and then divide by 2\n",
    "    scores = (act[:, :].T + 1) / 2\n",
    "    \n",
    "    img[1:, :, 0] = 255 * (1 - scores)\n",
    "    img[1:, :, 1] = 255 * scores\n",
    "\n",
    "    first_hs_img = img[:, :30, :]\n",
    "    second_hs_img = img[:, 30:60, :]\n",
    "    third_hs_img = img[:, 60:90, :]\n",
    "    fourth_hs_img = img[:, 90:, :]\n",
    "    \n",
    "    imgs = [first_hs_img,\n",
    "            second_hs_img,\n",
    "            third_hs_img,\n",
    "            fourth_hs_img]\n",
    "    \n",
    "    actual_imgs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        n_img_timesteps = img.shape[1]\n",
    "        \n",
    "        actual_imgs.append(get_image(img, n_img_timesteps, i))\n",
    "    \n",
    "    return actual_imgs\n",
    "\n",
    "example_sample_imgs = visualize_neurons(act)\n",
    "\n",
    "for img in example_sample_imgs:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpose-jupyter-data-exploration",
   "language": "python",
   "name": "openpose-jupyter-data-exploration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
